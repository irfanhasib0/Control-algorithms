{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan_hasib/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/irfan_hasib/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/irfan_hasib/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/irfan_hasib/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/irfan_hasib/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/irfan_hasib/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tensorflow as tf\n",
    "import json, sys, os\n",
    "from os import path\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "#env_to_use = 'Pendulum-v0'\n",
    "env_to_use = 'CarRacing-v0'\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "h1_actor = 8\n",
    "h2_actor = 8\n",
    "h3_actor = 8\n",
    "h1_critic = 8\n",
    "h2_critic = 8\n",
    "h3_critic = 8\n",
    "gamma = 0.99\n",
    "lr_actor = 1e-3\n",
    "lr_critic = 1e-3\n",
    "lr_decay = 1\n",
    "l2_reg_actor = 1e-6\n",
    "l2_reg_critic = 1e-6\n",
    "dropout_actor = 0\n",
    "dropout_critic = 0\n",
    "num_episodes = 150\n",
    "max_steps_ep = 10000\n",
    "tau = 1e-2\n",
    "train_every = 1\n",
    "replay_memory_capacity = int(1e5)\n",
    "minibatch_size = 1024\n",
    "initial_noise_scale = 0.1\n",
    "noise_decay = 0.99\n",
    "exploration_mu = 0.0\n",
    "exploration_theta = 0.15\n",
    "exploration_sigma = 0.2\n",
    "\n",
    "# game parameters\n",
    "env = gym.make(env_to_use)\n",
    "state_dim = env.observation_space.shape\n",
    "action_dim = np.prod(np.array(env.action_space.shape))\n",
    "\n",
    "# set seeds to 0\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=replay_memory_capacity)\n",
    "\n",
    "def add_to_memory(experience):\n",
    "    replay_memory.append(experience)\n",
    "\n",
    "def sample_from_memory(minibatch_size):\n",
    "    return random.sample(replay_memory, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class ANN():\n",
    "    tf.reset_default_graph()\n",
    "    state_ph  =  tf.placeholder(dtype=tf.float32, shape=[None,state_dim[0],state_dim[1],state_dim[2]])\n",
    "    action_ph = tf.placeholder(dtype=tf.float32, shape=[None,action_dim])\n",
    "    reward_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "    next_state_ph = tf.placeholder(dtype=tf.float32, shape=[None,state_dim[0],state_dim[1],state_dim[2]])\n",
    "    is_not_terminal_ph = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "    \n",
    "\n",
    "    \n",
    "    episodes = tf.Variable(0.0, trainable=False, name='episodes')\n",
    "    episode_inc_op = episodes.assign_add(1)\n",
    "    \n",
    "    def __init__(self):\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor_net_value = ANN.generate_actor_network(self,trainable = True, reuse = False)\n",
    "\n",
    "        \n",
    "        with tf.variable_scope('slow_target_actor', reuse=False):\n",
    "            self.target_actor_net_value = tf.stop_gradient(ANN.generate_actor_network(self,trainable = False, reuse = False))\n",
    "\n",
    "        with tf.variable_scope('critic') as scope:\n",
    "            self.critic_net_value = ANN.generate_critic_network(self,trainable = True, reuse = False)\n",
    "            self.q_value_for_actor_net = ANN.generate_critic_network(self,trainable = True, reuse = True,mode=2)\n",
    "\n",
    "        \n",
    "        with tf.variable_scope('slow_target_critic', reuse=False):\n",
    "            self.target_critic_net_value = tf.stop_gradient(ANN.generate_critic_network(self,trainable = False, reuse = False,mode=3))\n",
    "        \n",
    "        \n",
    "        self.actor_net_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
    "        self.target_actor_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='slow_target_actor')\n",
    "        self.critic_net_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic')\n",
    "        self.target_critic_net_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='slow_target_critic')\n",
    "\n",
    "        \n",
    "    def predict_graph(self):\n",
    "        return self.actor_net_value\n",
    "    def generate_actor_network(self,trainable, reuse):\n",
    "        layer1_out = tf.layers.conv2d(ANN.state_ph, filters=16, kernel_size=[8, 8],\n",
    "                                      strides=[4, 4], padding='same', activation=tf.nn.relu, data_format='channels_last', name='actor_layer1_out')\n",
    "        layer2_out = tf.layers.conv2d(layer1_out, filters=32, kernel_size=[4, 4],\n",
    "                                      strides=[2, 2], padding='same', activation=tf.nn.relu, data_format='channels_last', name='actor_layer2_out')\n",
    "        layer3_out = tf.layers.dense(tf.layers.flatten(layer2_out), 256, activation=tf.nn.relu, name='actor_layer3_out')\n",
    "        #output = tf.layers.dense(layer3_out, action_size, activation=None, name='output')\n",
    "        #hidden = tf.layers.dense(ANN.state_ph, h1_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense', reuse = reuse)\n",
    "        #hidden_2 = tf.layers.dense(hidden, h2_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense_1', reuse = reuse)\n",
    "        #hidden_3 = tf.layers.dense(hidden_2, h3_actor, activation = tf.nn.relu, trainable = trainable, name = 'dense_2', reuse = reuse)\n",
    "        actions_unscaled = tf.layers.dense(layer3_out, action_dim, trainable = trainable, name = 'dense_3', reuse = reuse)\n",
    "        actions = env.action_space.low + tf.nn.sigmoid(actions_unscaled)*(env.action_space.high - env.action_space.low) # bound the actions to the valid range\n",
    "        return actions\n",
    "\n",
    "   \n",
    "    \n",
    "    def generate_critic_network(self,trainable, reuse,mode=1):\n",
    "        layer1_out = tf.layers.conv2d(ANN.state_ph, filters=16, kernel_size=[8, 8],\n",
    "                                      strides=[4, 4], padding='same', activation=tf.nn.relu, data_format='channels_last', name='critic_layer1_out',reuse = reuse)\n",
    "        layer2_out = tf.layers.conv2d(layer1_out, filters=32, kernel_size=[4, 4],\n",
    "                                      strides=[2, 2], padding='same', activation=tf.nn.relu, data_format='channels_last', name='critic_layer2_out',reuse = reuse)\n",
    "        layer2_flat=tf.layers.flatten(layer2_out)\n",
    "        if mode==1:\n",
    "            state_action = tf.concat([layer2_flat, ANN.action_ph], axis=1)\n",
    "        if mode==2:\n",
    "            state_action = tf.concat([layer2_flat,self.actor_net_value], axis=1)\n",
    "        if mode==3:\n",
    "            state_action = tf.concat([layer2_flat,self.target_actor_net_value], axis=1)\n",
    "        layer3_out = tf.layers.dense(state_action, 256, activation=tf.nn.relu, name='critic_layer3_out',reuse = reuse)\n",
    "        #hidden = tf.layers.dense(state_action, h1_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense', reuse = reuse)\n",
    "        #hidden_2 = tf.layers.dense(hidden, h2_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense_1', reuse = reuse)\n",
    "        #hidden_3 = tf.layers.dense(hidden_2, h3_critic, activation = tf.nn.relu, trainable = trainable, name = 'dense_2', reuse = reuse)\n",
    "        q_values = tf.layers.dense(layer3_out, 1, trainable = trainable, name = 'dense_3', reuse = reuse)\n",
    "        return q_values\n",
    "    def train_graph(self):\n",
    "        updated_q_values = tf.expand_dims(ANN.reward_ph, 1) + tf.expand_dims(ANN.is_not_terminal_ph, 1) * gamma * self.target_critic_net_value\n",
    "        td_errors = updated_q_values - self.critic_net_value\n",
    "\n",
    "\n",
    "        critic_loss = tf.reduce_mean(tf.square(td_errors))\n",
    "        for var in self.critic_net_vars:\n",
    "            if not 'bias' in var.name:\n",
    "                critic_loss += l2_reg_critic * 0.5 * tf.nn.l2_loss(var)\n",
    "\n",
    "\n",
    "        critic_train_op = tf.train.AdamOptimizer(lr_critic).minimize(critic_loss)\n",
    "        actor_loss = -1*tf.reduce_mean(self.q_value_for_actor_net)\n",
    "        for var in self.actor_net_vars:\n",
    "            if not 'bias' in var.name:\n",
    "                actor_loss += l2_reg_actor * 0.5 * tf.nn.l2_loss(var)\n",
    "\n",
    "        actor_train_op = tf.train.AdamOptimizer(lr_actor).minimize(actor_loss, var_list=self.actor_net_vars)\n",
    "        return actor_train_op,critic_train_op\n",
    "        \n",
    "    def update_wts_graph(self):\n",
    "            update_slow_target_ops = []\n",
    "            for i, target_actor_var in enumerate(self.target_actor_net_vars):\n",
    "                update_slow_target_actor_op = target_actor_var.assign(tau*self.actor_net_vars[i]+(1-tau)*target_actor_var)\n",
    "                update_slow_target_ops.append(update_slow_target_actor_op)\n",
    "\n",
    "            for i, slow_target_var in enumerate(self.target_critic_net_vars):\n",
    "                update_slow_target_critic_op = slow_target_var.assign(tau*self.critic_net_vars[i]+(1-tau)*slow_target_var)\n",
    "                update_slow_target_ops.append(update_slow_target_critic_op)\n",
    "\n",
    "            update_slow_targets_op = tf.group(*update_slow_target_ops, name='update_slow_targets')\n",
    "            return update_slow_targets_op "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=ANN()\n",
    "actor_net_value=Model.predict_graph()\n",
    "actor_train_op,critic_train_op=Model.train_graph()\n",
    "update_wts_op=Model.update_wts_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# initialize session\n",
    "sess = tf.Session()\t\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1143..1442 -> 299-tiles track\n",
      "Episode  0, Reward: -93.289, Steps: 1000, noise:   0.200\n",
      "Track generation: 1087..1369 -> 282-tiles track\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################################\n",
    "## Training\n",
    "\n",
    "total_steps = 0\n",
    "log_rewards=[]\n",
    "for ep in range(num_episodes):\n",
    "\n",
    "    total_reward = 0\n",
    "    steps_in_ep = 0\n",
    "\n",
    "    \n",
    "    noise_process = np.zeros(action_dim)\n",
    "    noise_scale = (initial_noise_scale * noise_decay**ep) * (env.action_space.high - env.action_space.low)\n",
    "\n",
    "    \n",
    "    observation = env.reset()\n",
    "    for t in range(max_steps_ep):\n",
    "\n",
    "        action_for_state, = sess.run(actor_net_value, \n",
    "            feed_dict = {Model.state_ph: observation[None]})\n",
    "\n",
    "        \n",
    "        noise_process = exploration_theta*(exploration_mu - noise_process) + exploration_sigma*np.random.randn(action_dim)\n",
    "        action_for_state += noise_scale*noise_process\n",
    "\n",
    "       \n",
    "        next_observation, reward, done, _info = env.step(action_for_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        add_to_memory((observation, action_for_state, reward, next_observation, \n",
    "            0.0 if done else 1.0))\n",
    "        \n",
    "        if total_steps%train_every == 0 and len(replay_memory) >= minibatch_size:\n",
    "\n",
    "           \n",
    "            minibatch = sample_from_memory(minibatch_size)\n",
    "\n",
    "            _, _ = sess.run([critic_train_op, actor_train_op], \n",
    "                feed_dict = {\n",
    "                    Model.state_ph: np.asarray([elem[0] for elem in minibatch]),\n",
    "                    Model.action_ph: np.asarray([elem[1] for elem in minibatch]),\n",
    "                    Model.reward_ph: np.asarray([elem[2] for elem in minibatch]),\n",
    "                    Model.next_state_ph: np.asarray([elem[3] for elem in minibatch]),\n",
    "                    Model.is_not_terminal_ph: np.asarray([elem[4] for elem in minibatch]),\n",
    "                    })\n",
    "\n",
    "\n",
    "            _ = sess.run(update_wts_op)\n",
    "\n",
    "        observation = next_observation\n",
    "        total_steps += 1\n",
    "        steps_in_ep += 1\n",
    "        \n",
    "        if done: \n",
    "            \n",
    "            _ = sess.run(Model.episode_inc_op)\n",
    "            break\n",
    "    log_rewards.append([ep,total_reward])\n",
    "    print('Episode %2i, Reward: %7.3f, Steps: %i, noise: %7.3f'%(ep,total_reward,steps_in_ep, noise_scale[0]))\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "env=gym.make(env_to_use)\n",
    "env.seed(0)\n",
    "obs=env.reset()\n",
    "\n",
    "#os.mkdir(env_to_use+'Test')\n",
    "#os.mkdir(env_to_use+'Test/img/')\n",
    "for i in range(200):\n",
    "    \n",
    "    _action, = sess.run(actor_net_value, \n",
    "                    feed_dict = {Model.state_ph: obs[None]})\n",
    "    obs,rew,done,info=env.step(_action)\n",
    "    img=env.render(mode='rgb_array')\n",
    "    time.sleep(0.1)\n",
    "    #cv2.imwrite(env_to_use+'Test/img/'+str(i)+'.jpg',img)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "rws=np.array(log_rewards)\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "plt.plot(rws[:,0],rws[:,1])\n",
    "plt.title('epoch vs sum of reward')\n",
    "plt.savefig(env_to_use+'Test/'+'rewards.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(rws[:,1],columns=['sum_of_rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(env_to_use+'Test/rewards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
