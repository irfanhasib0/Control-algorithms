{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Basic_Architecture:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layer_sizes = [32, 32]\n",
    "\n",
    "    def evaluate(self, input, action_size):\n",
    "        neural_net = input\n",
    "        for n in self.layer_sizes:\n",
    "            neural_net = tf.layers.dense(neural_net, n, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(neural_net, action_size, activation=None, name='output')\n",
    "        return output\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"2 dense layers of size {0} and {1}\".format(self.layer_sizes[0], self.layer_sizes[1])\n",
    "\n",
    "\n",
    "# A class that defines a neural network with the following architecture:\n",
    "# - 1 convolutional layer with 16 8x8 kernels with a stride of 4x4 w/ ReLU activation\n",
    "# - 1 fully connected layer with 16 neurons and ReLU activation. Dropout is applied with\n",
    "#   keep probability of .8\n",
    "class Conv_1Layer:\n",
    "\n",
    "    def evaluate(self, input, action_size):\n",
    "        layer1_out = tf.layers.conv2d(input, filters=16, kernel_size=[8, 8],\n",
    "                                      strides=[4, 4], padding='same', activation=tf.nn.relu, data_format='channels_first', name='layer1_out')\n",
    "        layer1_shape = np.prod(np.shape(layer1_out)[1:])\n",
    "        layer2_out = tf.nn.dropout(tf.layers.dense(tf.reshape(layer1_out, [-1, layer1_shape]), 16, activation=tf.nn.relu), .8, name='layer2_out')\n",
    "        output = tf.layers.dense(layer2_out, action_size, activation=None, name='output')\n",
    "        return output\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"1 convolutional layer: filters = 16, kernel = [8,8], strides = [4,4], relu activation and one dense dropout layer with drop probability 20\\% and 16 neurons\"\n",
    "\n",
    "\n",
    "# A class that defines a neural network with the following architecture:\n",
    "# - 1 convolutional layer with 16 8x8 kernels with a stride of 4x4 w/ ReLU activation\n",
    "# - 1 convolutional layer with 32 4x4 kernels with a stride of 2x2 w/ ReLU activation\n",
    "# - 1 fully connected layer with 256 neurons and ReLU activation. Dropout is applied with\n",
    "#   keep probability of .7\n",
    "class Conv_2Layer:\n",
    "\n",
    "    def evaluate(self, input, action_size):\n",
    "        layer1_out = tf.layers.conv2d(input, filters=16, kernel_size=[8, 8],\n",
    "                                      strides=[4, 4], padding='same', activation=tf.nn.relu, data_format='channels_first', name='layer1_out')\n",
    "        layer2_out = tf.layers.conv2d(layer1_out, filters=32, kernel_size=[4, 4],\n",
    "                                      strides=[2, 2], padding='same', activation=tf.nn.relu, data_format='channels_first', name='layer2_out')\n",
    "        layer2_out = tf.layers.flatten(layer2_out)\n",
    "        layer3_out = tf.nn.dropout(tf.layers.dense(layer2_out, 256, activation=tf.nn.relu), 0.7, name='layer3_out')\n",
    "        output = tf.layers.dense(layer3_out, action_size, activation=None, name='output')\n",
    "        return output\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"conv2 with dropout 0.7\"\n",
    "\n",
    "\n",
    "# A class that defines a neural network with the following architecture:\n",
    "# - 1 convolutional layer with 16 8x8 kernels with a stride of 4x4 w/ ReLU activation\n",
    "# - 1 convolutional layer with 32 4x4 kernels with a stride of 2x2 w/ ReLU activation\n",
    "# - 1 fully connected layer with 256 neurons and ReLU activation. \n",
    "# Based on 2013 paper 'Playing Atari with Deep Reinforcement Learning' by Mnih et al\n",
    "class Atari_Paper:\n",
    "\n",
    "    def evaluate(self, input, action_size):\n",
    "        layer1_out = tf.layers.conv2d(input, filters=16, kernel_size=[8, 8],\n",
    "                                      strides=[4, 4], padding='same', activation=tf.nn.relu, data_format='channels_first', name='layer1_out')\n",
    "        layer2_out = tf.layers.conv2d(layer1_out, filters=32, kernel_size=[4, 4],\n",
    "                                      strides=[2, 2], padding='same', activation=tf.nn.relu, data_format='channels_first', name='layer2_out')\n",
    "        layer3_out = tf.layers.dense(tf.layers.flatten(layer2_out), 256, activation=tf.nn.relu, name='layer3_out')\n",
    "        output = tf.layers.dense(layer3_out, action_size, activation=None, name='output')\n",
    "        return output\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Architecture used in the atari paper\"\n",
    "\n",
    "# A class that defines a neural network with the following architecture:\n",
    "# - 1 convolutional layer with 32 8x8 kernels with a stride of 4x4 w/ ReLU activation\n",
    "# - 1 convolutional layer with 64 4x4 kernels with a stride of 2x2 w/ ReLU activation\n",
    "# - 1 convolutional layer with 64 3x3 kernels with a stride of 2x2 w/ ReLU activation\n",
    "# - 1 fully connected layer with 512 neurons and ReLU activation. \n",
    "# Based on 2015 paper 'Human-level control through deep reinforcement learning' by Mnih et al\n",
    "class Nature_Paper:\n",
    "\n",
    "    def evaluate(self, input, action_size):\n",
    "        layer1_out = tf.layers.conv2d(input, filters=32, kernel_size=[8,8],\n",
    "            strides=[4,4], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer1_out')\n",
    "        layer2_out = tf.layers.conv2d(layer1_out, filters=64, kernel_size=[4,4],\n",
    "            strides=[2,2], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer2_out')\n",
    "        layer3_out = tf.layers.conv2d(layer2_out, filters=64, kernel_size=[3,3],\n",
    "            strides=[1,1], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer3_out')\n",
    "        layer4_out = tf.layers.dense(tf.layers.flatten(layer3_out), 512, activation=tf.nn.relu, name='layer4_out')\n",
    "        output =  tf.layers.dense(layer4_out, action_size, activation=None, name='output')\n",
    "        return output\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Architecture used in the nature paper in 2015\"\n",
    "\n",
    "# A class that defines a neural network with the following architecture:\n",
    "# - 1 convolutional layer with 32 8x8 kernels with a stride of 4x4 w/ ReLU activation\n",
    "# - 1 convolutional layer with 64 4x4 kernels with a stride of 2x2 w/ ReLU activation\n",
    "# - 1 convolutional layer with 64 3x3 kernels with a stride of 2x2 w/ ReLU activation\n",
    "# - 1 fully connected layer with 512 neurons and ReLU activation. \n",
    "# - Same as 'Nature_Paper' but with batchnorm on output layer\n",
    "# Based on 2015 paper 'Human-level control through deep reinforcement learning' by Mnih et al\n",
    "class Nature_Paper_Batchnorm:\n",
    "\n",
    "    def evaluate(self, input, action_size):\n",
    "        layer1_out = tf.contrib.layers.batch_norm(tf.layers.conv2d(input, filters=32, kernel_size=[8,8],\n",
    "            strides=[4,4], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer1_out'), data_format='NCHW')\n",
    "        layer2_out = tf.contrib.layers.batch_norm(tf.layers.conv2d(layer1_out, filters=64, kernel_size=[4,4],\n",
    "            strides=[2,2], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer2_out'), data_format='NCHW')\n",
    "        layer3_out = tf.contrib.layers.batch_norm(tf.layers.conv2d(layer2_out, filters=64, kernel_size=[3,3],\n",
    "            strides=[1,1], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer3_out'), data_format='NCHW')\n",
    "        layer4_out = tf.contrib.layers.batch_norm(tf.layers.dense(tf.layers.flatten(layer3_out), 512, activation=tf.nn.relu, name='layer4_out'))\n",
    "        output =  tf.contrib.layers.batch_norm(tf.layers.dense(layer4_out, action_size, activation=None, name='output'))\n",
    "        return output\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Architecture used in the nature paper in 2015 with batchnorm on every layer\"\n",
    "\n",
    "# A class that defines a neural network with the following architecture:\n",
    "# - 1 convolutional layer with 32 8x8 kernels with a stride of 4x4 w/ ReLU activation\n",
    "# - 1 convolutional layer with 64 4x4 kernels with a stride of 2x2 w/ ReLU activation\n",
    "# - 1 convolutional layer with 64 3x3 kernels with a stride of 2x2 w/ ReLU activation\n",
    "# - 1 fully connected layer with 512 neurons and ReLU activation. \n",
    "# - Same as 'Nature_Paper' but with dropout with keep probability .7 on output layer\n",
    "# Based on 2015 paper 'Human-level control through deep reinforcement learning' by Mnih et al\n",
    "class Nature_Paper_Dropout:\n",
    "\n",
    "    def evaluate(self, input, action_size):\n",
    "        layer1_out = tf.layers.conv2d(input, filters=32, kernel_size=[8,8],\n",
    "            strides=[4,4], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer1_out')\n",
    "        layer2_out = tf.layers.conv2d(layer1_out, filters=64, kernel_size=[4,4],\n",
    "            strides=[2,2], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer2_out')\n",
    "        layer3_out = tf.layers.conv2d(layer2_out, filters=64, kernel_size=[3,3],\n",
    "            strides=[1,1], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer3_out')\n",
    "        layer4_out = tf.nn.dropout(tf.layers.dense(tf.layers.flatten(layer3_out), 512, activation=tf.nn.relu), .7, name='layer4_out')\n",
    "        output =  tf.layers.dense(layer4_out, action_size, activation=None, name='output')\n",
    "        return output\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Architecture used in the nature paper in 2015 with dropout on dense layers, 0.7 keep prob.\"\n",
    "\n",
    "# A class that defines a neural network with the following architecture:\n",
    "# - 1 convolutional layer with 32 8x8 kernels with a stride of 4x4 w/ ReLU activation\n",
    "# - 1 convolutional layer with 64 4x4 kernels with a stride of 2x2 w/ ReLU activation\n",
    "# - 1 convolutional layer with 64 3x3 kernels with a stride of 2x2 w/ ReLU activation\n",
    "# - 1 fully connected layer with 512 neurons and ReLU activation. \n",
    "# - Same as 'Nature_Paper' but with dropout with keep probability .5 on 2nd convolutional layer\n",
    "# Based on 2015 paper 'Human-level control through deep reinforcement learning' by Mnih et al\n",
    "class Nature_Paper_Conv_Dropout:\n",
    "\n",
    "    def evaluate(self, input, action_size):\n",
    "        layer1_out = tf.layers.conv2d(input, filters=32, kernel_size=[8,8],\n",
    "            strides=[4,4], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer1_out')\n",
    "        layer2_out = tf.nn.dropout(tf.layers.conv2d(layer1_out, filters=64, kernel_size=[4,4],\n",
    "            strides=[2,2], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer2_out'), .5, name='layer2_out')\n",
    "        layer3_out = tf.layers.conv2d(layer2_out, filters=64, kernel_size=[3,3],\n",
    "            strides=[1,1], padding='same', activation=tf.nn.relu, data_format='channels_first', \n",
    "            kernel_initializer=tf.contrib.layers.xavier_initializer(), name='layer3_out')\n",
    "        layer4_out = tf.layers.dense(tf.layers.flatten(layer3_out), 512, activation=tf.nn.relu)\n",
    "        output =  tf.layers.dense(layer4_out, action_size, activation=None)\n",
    "        return output\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Architecture used in the nature paper in 2015 with dropout on 2nd conv layer, 0.7 keep prob.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
